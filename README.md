# 实验一：基于神经网络的新闻文本分类

## 一、实验目的

* 熟悉文本分类的基本原理；
* 熟悉神经网络训练过程及相关参数；
* 学会评估神经网络模型的性能；
* 了解相关库的使用方法。

## 二、实验原理

### 1. 数据集介绍

fetch_20newsgroups 是一个在机器学习库 scikit-learn 中用于加载 20 个新闻组数据集的函数。这个数据集包含了大约 18000 篇新闻组文章，涵盖了20 个不同的主题，因此被称为 20 newsgroups 数据集。<br>

> 通过`from sklearn.datasets import fetch_20newsgroups`可以获取数据集。调用数据集中的属性`target_names`可以观察到20个主题名称，`data`属性获取原文信息。

### 2. 数据预处理

原文为字符串形式，需要进行预处理，将其转换为数字形式，以便于神经网络模型的训练。sklearn库中采用TF-IDF（Term Frequency-Inverse Document Frequency）算法进行文本预处理。

> TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度，其中TF是词频(Term Frequency)，IDF是逆文本频率指数(Inverse Document Frequency)。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。    ——《百度百科》

$ TF-IDF(t,d)=TF(t,d) * IDF(t)=\frac{n_{t,d}}{N_d} * log\frac{N}{n_t} $

其中$n_{t,d}$表示词t在d文档出现次数，$N_d$表示文档d中所有词的出现次数总和，$N$是语料库中文档总数，$n_t$表示包含词t的文档数。
通过该公式可以将该数据集中所有词统计得到一个数字向量，在`sklearn`库中使用`TfidfVectorizer().fit_transform()`函数即可完成转换。通过观察得到的数据矩阵`(18846, 173762)`可以判断其包含了18000篇文章的173762个词的TF-IDF值。


## 实验内容

> 1. 实验环境配置：本实验基于Python3.10，主要使用scikit-learn库获取数据集及配置神经网络模型，使用matplot库绘制图表。
> 2. 数据集：使用新闻文本分类数据集，使用fetch_20newsgroups作为数据集。
> 3. 

